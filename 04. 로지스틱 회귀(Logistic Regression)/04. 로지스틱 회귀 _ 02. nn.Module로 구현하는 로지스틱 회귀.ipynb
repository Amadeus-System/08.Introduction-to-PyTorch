{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04. 로지스틱 회귀 / 02. nn.Module로 구현하는 로지스틱 회귀.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMzFqJw6btp5TkUYeZrkEgF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y7IK048PBFhy"},"source":["## 02.nn.Module로 구현하는 로지스틱 회귀\n","\n","잠깐만 복습을 해보면 선형 회귀 모델의 가설식은 $H(x) = Wx + b$이었다. 그리고 이 가설식을 구현하기 위해서 파이토치의 nn.Linear()를 사용했었다. 그리고 로지스틱 회귀의 가설식은 $H(x) = sigmoid(Wx + b)$이다. 파이토치에서는 nn.Sigmoid()를 통해서 시그모이드 함수를 구현하므로 결과적으로 nn.Linear()의 결과를 nn.Sigmoid()를 거치게하면 로지스틱 회귀의 가설식이 된다.\n","\n","파이토치를 통해 이를 구현해보자."]},{"cell_type":"markdown","metadata":{"id":"kDQ6CbfTBJPG"},"source":["### 1.파이토치의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기\n","\n","우선 구현을 위해 필요한 파이토치의 도구들을 임포트한다."]},{"cell_type":"code","metadata":{"id":"FkoapOLMBJRN","executionInfo":{"status":"ok","timestamp":1606989388927,"user_tz":-540,"elapsed":3717,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1693f0CBJTT","executionInfo":{"status":"ok","timestamp":1606989388928,"user_tz":-540,"elapsed":3712,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"66698705-85a4-43c1-d5f6-8a0803f32e8c"},"source":["torch.manual_seed(1)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fee35f62b10>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"qcFKCxpaBJVf"},"source":["훈련 데이터를 텐서로 선언한다."]},{"cell_type":"code","metadata":{"id":"5p9EvWuCBJXl","executionInfo":{"status":"ok","timestamp":1606989415416,"user_tz":-540,"elapsed":1206,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1], [1]]\n","\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SU3XxcMABJZe"},"source":["nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 한다. 뒤에서 이를 이용해서 인공 신경망을 구현하게 되므로 기억하고 있으면 좋다. 조금 쉽게 말해서 nn.Sequential()은 $Wx + b$와 같은 수식과 시그모이드 함수 등과 같은 여러 함수들을 연결해주는 역할을 한다. 이를 이용해서 로지스틱 회귀를 구현해보자."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7DV4YnfCJPR","executionInfo":{"status":"ok","timestamp":1606989552288,"user_tz":-540,"elapsed":1022,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"2561b691-26ce-485f-eabb-771acc178a0c"},"source":["model = nn.Sequential(\n","    nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n","    nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",")\n","\n","model"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=2, out_features=1, bias=True)\n","  (1): Sigmoid()\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"auJBpzo4CJRZ"},"source":["현재 W와 b는 랜덤 초기화가 된 상태이다. 훈련 데이터를 넣어 예측값을 확인해보자."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olCYBDd4CJT9","executionInfo":{"status":"ok","timestamp":1606989590380,"user_tz":-540,"elapsed":1038,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"4048feff-4427-4a05-dab1-1777d95e5c8d"},"source":["model(x_train)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4020],\n","        [0.4147],\n","        [0.6556],\n","        [0.5948],\n","        [0.6788],\n","        [0.8061]], grad_fn=<SigmoidBackward>)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Pr0_dnq1CJWX"},"source":["6 x 1 크기의 예측값 텐서가 출력된다. 그러나 현재 W와 b는 임의의 값을 가지므로 현재의 예측은 의미가 없다. 이제 경사 하강법을 사용하여 훈련해보겠다. 총 100번의 에포크를 수행한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7ym2tYAC_U7","executionInfo":{"status":"ok","timestamp":1606989667133,"user_tz":-540,"elapsed":1040,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"d841e541-767c-44ba-e802-9e44c5be8dd4"},"source":["# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr = 1)\n","optimizer"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 1\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giph2L9QDFWR","executionInfo":{"status":"ok","timestamp":1606989901700,"user_tz":-540,"elapsed":1437,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"bf2a31ef-835b-4536-bdd1-ef620e1b0ca1"},"source":["nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = model(x_train)\n","\n","    # cost 계산\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 20번마다 로그 출력\n","    if epoch % 10 == 0:\n","        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n","\n","        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n","\n","        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n","\n","        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy: {:2.2f}%'.format(epoch, nb_epochs,\n","                                                                      cost.item(), accuracy * 100))\n","        "],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch    0/1000 Cost: 0.539713 Accuracy: 83.33%\n","Epoch   10/1000 Cost: 0.614853 Accuracy: 66.67%\n","Epoch   20/1000 Cost: 0.441875 Accuracy: 66.67%\n","Epoch   30/1000 Cost: 0.373145 Accuracy: 83.33%\n","Epoch   40/1000 Cost: 0.316358 Accuracy: 83.33%\n","Epoch   50/1000 Cost: 0.266094 Accuracy: 83.33%\n","Epoch   60/1000 Cost: 0.220498 Accuracy: 100.00%\n","Epoch   70/1000 Cost: 0.182095 Accuracy: 100.00%\n","Epoch   80/1000 Cost: 0.157299 Accuracy: 100.00%\n","Epoch   90/1000 Cost: 0.144091 Accuracy: 100.00%\n","Epoch  100/1000 Cost: 0.134272 Accuracy: 100.00%\n","Epoch  110/1000 Cost: 0.125769 Accuracy: 100.00%\n","Epoch  120/1000 Cost: 0.118297 Accuracy: 100.00%\n","Epoch  130/1000 Cost: 0.111680 Accuracy: 100.00%\n","Epoch  140/1000 Cost: 0.105779 Accuracy: 100.00%\n","Epoch  150/1000 Cost: 0.100483 Accuracy: 100.00%\n","Epoch  160/1000 Cost: 0.095704 Accuracy: 100.00%\n","Epoch  170/1000 Cost: 0.091369 Accuracy: 100.00%\n","Epoch  180/1000 Cost: 0.087420 Accuracy: 100.00%\n","Epoch  190/1000 Cost: 0.083806 Accuracy: 100.00%\n","Epoch  200/1000 Cost: 0.080486 Accuracy: 100.00%\n","Epoch  210/1000 Cost: 0.077425 Accuracy: 100.00%\n","Epoch  220/1000 Cost: 0.074595 Accuracy: 100.00%\n","Epoch  230/1000 Cost: 0.071969 Accuracy: 100.00%\n","Epoch  240/1000 Cost: 0.069526 Accuracy: 100.00%\n","Epoch  250/1000 Cost: 0.067248 Accuracy: 100.00%\n","Epoch  260/1000 Cost: 0.065118 Accuracy: 100.00%\n","Epoch  270/1000 Cost: 0.063122 Accuracy: 100.00%\n","Epoch  280/1000 Cost: 0.061247 Accuracy: 100.00%\n","Epoch  290/1000 Cost: 0.059483 Accuracy: 100.00%\n","Epoch  300/1000 Cost: 0.057820 Accuracy: 100.00%\n","Epoch  310/1000 Cost: 0.056250 Accuracy: 100.00%\n","Epoch  320/1000 Cost: 0.054764 Accuracy: 100.00%\n","Epoch  330/1000 Cost: 0.053357 Accuracy: 100.00%\n","Epoch  340/1000 Cost: 0.052022 Accuracy: 100.00%\n","Epoch  350/1000 Cost: 0.050753 Accuracy: 100.00%\n","Epoch  360/1000 Cost: 0.049546 Accuracy: 100.00%\n","Epoch  370/1000 Cost: 0.048396 Accuracy: 100.00%\n","Epoch  380/1000 Cost: 0.047299 Accuracy: 100.00%\n","Epoch  390/1000 Cost: 0.046252 Accuracy: 100.00%\n","Epoch  400/1000 Cost: 0.045251 Accuracy: 100.00%\n","Epoch  410/1000 Cost: 0.044294 Accuracy: 100.00%\n","Epoch  420/1000 Cost: 0.043376 Accuracy: 100.00%\n","Epoch  430/1000 Cost: 0.042497 Accuracy: 100.00%\n","Epoch  440/1000 Cost: 0.041653 Accuracy: 100.00%\n","Epoch  450/1000 Cost: 0.040843 Accuracy: 100.00%\n","Epoch  460/1000 Cost: 0.040064 Accuracy: 100.00%\n","Epoch  470/1000 Cost: 0.039315 Accuracy: 100.00%\n","Epoch  480/1000 Cost: 0.038593 Accuracy: 100.00%\n","Epoch  490/1000 Cost: 0.037898 Accuracy: 100.00%\n","Epoch  500/1000 Cost: 0.037228 Accuracy: 100.00%\n","Epoch  510/1000 Cost: 0.036582 Accuracy: 100.00%\n","Epoch  520/1000 Cost: 0.035958 Accuracy: 100.00%\n","Epoch  530/1000 Cost: 0.035356 Accuracy: 100.00%\n","Epoch  540/1000 Cost: 0.034773 Accuracy: 100.00%\n","Epoch  550/1000 Cost: 0.034210 Accuracy: 100.00%\n","Epoch  560/1000 Cost: 0.033664 Accuracy: 100.00%\n","Epoch  570/1000 Cost: 0.033137 Accuracy: 100.00%\n","Epoch  580/1000 Cost: 0.032625 Accuracy: 100.00%\n","Epoch  590/1000 Cost: 0.032130 Accuracy: 100.00%\n","Epoch  600/1000 Cost: 0.031649 Accuracy: 100.00%\n","Epoch  610/1000 Cost: 0.031183 Accuracy: 100.00%\n","Epoch  620/1000 Cost: 0.030730 Accuracy: 100.00%\n","Epoch  630/1000 Cost: 0.030291 Accuracy: 100.00%\n","Epoch  640/1000 Cost: 0.029864 Accuracy: 100.00%\n","Epoch  650/1000 Cost: 0.029449 Accuracy: 100.00%\n","Epoch  660/1000 Cost: 0.029046 Accuracy: 100.00%\n","Epoch  670/1000 Cost: 0.028654 Accuracy: 100.00%\n","Epoch  680/1000 Cost: 0.028272 Accuracy: 100.00%\n","Epoch  690/1000 Cost: 0.027900 Accuracy: 100.00%\n","Epoch  700/1000 Cost: 0.027538 Accuracy: 100.00%\n","Epoch  710/1000 Cost: 0.027186 Accuracy: 100.00%\n","Epoch  720/1000 Cost: 0.026842 Accuracy: 100.00%\n","Epoch  730/1000 Cost: 0.026507 Accuracy: 100.00%\n","Epoch  740/1000 Cost: 0.026181 Accuracy: 100.00%\n","Epoch  750/1000 Cost: 0.025862 Accuracy: 100.00%\n","Epoch  760/1000 Cost: 0.025552 Accuracy: 100.00%\n","Epoch  770/1000 Cost: 0.025248 Accuracy: 100.00%\n","Epoch  780/1000 Cost: 0.024952 Accuracy: 100.00%\n","Epoch  790/1000 Cost: 0.024663 Accuracy: 100.00%\n","Epoch  800/1000 Cost: 0.024381 Accuracy: 100.00%\n","Epoch  810/1000 Cost: 0.024104 Accuracy: 100.00%\n","Epoch  820/1000 Cost: 0.023835 Accuracy: 100.00%\n","Epoch  830/1000 Cost: 0.023571 Accuracy: 100.00%\n","Epoch  840/1000 Cost: 0.023313 Accuracy: 100.00%\n","Epoch  850/1000 Cost: 0.023061 Accuracy: 100.00%\n","Epoch  860/1000 Cost: 0.022814 Accuracy: 100.00%\n","Epoch  870/1000 Cost: 0.022572 Accuracy: 100.00%\n","Epoch  880/1000 Cost: 0.022336 Accuracy: 100.00%\n","Epoch  890/1000 Cost: 0.022104 Accuracy: 100.00%\n","Epoch  900/1000 Cost: 0.021877 Accuracy: 100.00%\n","Epoch  910/1000 Cost: 0.021655 Accuracy: 100.00%\n","Epoch  920/1000 Cost: 0.021437 Accuracy: 100.00%\n","Epoch  930/1000 Cost: 0.021224 Accuracy: 100.00%\n","Epoch  940/1000 Cost: 0.021015 Accuracy: 100.00%\n","Epoch  950/1000 Cost: 0.020810 Accuracy: 100.00%\n","Epoch  960/1000 Cost: 0.020609 Accuracy: 100.00%\n","Epoch  970/1000 Cost: 0.020412 Accuracy: 100.00%\n","Epoch  980/1000 Cost: 0.020219 Accuracy: 100.00%\n","Epoch  990/1000 Cost: 0.020029 Accuracy: 100.00%\n","Epoch 1000/1000 Cost: 0.019843 Accuracy: 100.00%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GUGdEBN6D-ml"},"source":["각 에포크마다 정확도를 계산하여 정확도도 출력해보겠다.\n","\n","중간부터 정확도는 100%가 나오기 시작한다. 기존의 훈련 데이터를 입력하여 예측값을 확인해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3m3r_S-dEMYL","executionInfo":{"status":"ok","timestamp":1606989964937,"user_tz":-540,"elapsed":1055,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"e7eaf8f3-3cd9-4431-85a9-858f45bfb6f6"},"source":["model(x_train)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2.7616e-04],\n","        [3.1595e-02],\n","        [3.8959e-02],\n","        [9.5624e-01],\n","        [9.9823e-01],\n","        [9.9969e-01]], grad_fn=<SigmoidBackward>)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"vobeCULDEN4c"},"source":["0.5를 넘으면 True, 그보다 낮으면 False로 간주한다. 실제값은 [[0], [0], [0], [1], [1], [1]]이다. 이는 False, False, False, True, True, True에 해당되므로 전부 실제값과 일치하도록 예측한 것을 확인할 수 있다.\n","\n","훈련 후의 W와 b의 값을 출력해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAbDP_w8EfuY","executionInfo":{"status":"ok","timestamp":1606990044167,"user_tz":-540,"elapsed":1206,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"35afe849-7e2c-45c9-9176-a6ddc480381e"},"source":["print(list(model.parameters()))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[Parameter containing:\n","tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:\n","tensor([-14.4839], requires_grad=True)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q0mOB5o-Ehcp"},"source":["출력값이 앞 챕터에서 nn.Module을 사용하지 않고 로지스틱 회귀를 구현한 실습에서 얻었던 W와 b와 거의 일치한다."]},{"cell_type":"markdown","metadata":{"id":"uGWw855KEqMd"},"source":["### 2.인공 신경망으로 표현되는 로지스틱 회귀.\n","\n","사실 로지스틱 회귀는 인공 신경망으로 간주할 수 있다.\n","\n","<img src = 'https://wikidocs.net/images/page/58686/logistic_regression.PNG' width = 60%>\n","\n","위의 인공 신경망 그림에서 각 화살표는 입력과 곱해지는 가중치 또는 편향이다. 각 입력에 대해서 검은색 화살표는 가중치, 회색 화살표는 편향이 곱해진다. 각 입력 $x$는 각 입력의 가중치 $w$와 곱해지고, 편향 $b$는 상수 1과 곱해지는 것으로 표현되었다. 그리고 출력하기 전에 시그모이드 함수를 지나게 된다.\n","\n","결과적으로 위의 인공 신경망은 다음과 같은 다중 로지스틱 회귀를 표현하고 있다.\n","\n","$H(x)=sigmoid(x_{1}w_{1} + x_{2}w_{2} + b)$"]},{"cell_type":"markdown","metadata":{"id":"la5dR5TfFrGe"},"source":["* **뒤에서 인공 신경망을 배우면서 언급하겠지만, 시그모이드 함수는 인공 신경망의 은닉층에서는 거의 사용되지 않는다.**\n","\n","로지스틱 회귀와 소프트맥스 회귀 : https://hyeonnii.tistory.com/239"]}]}