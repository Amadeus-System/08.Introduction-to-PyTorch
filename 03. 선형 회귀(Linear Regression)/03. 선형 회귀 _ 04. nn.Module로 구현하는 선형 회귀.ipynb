{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03. 선형 회귀 / 04. nn.Module로 구현하는 선형 회귀.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOgq8rRb+hQunroGWOOuE2v"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RR3KYr6M-YKZ"},"source":["## 04. nn.Module로 구현하는 선형 회귀\n","\n","이전 챕터까지는 선형 회귀를 좀 더 직접적으로 이해하기 위해 가설, 비용 함수를 직접 정의해서 선형 회귀 모델을 구현하였다. 이번에는 파이토치에서 이미 구현되어져 제공되고 있는 함수들을 불러오는 것으로 더 쉽게 선형 회귀 모델을 구현해보겠다.\n","\n","예를 들어 파이토치에서는 선형 회귀 모델이 nn.Linear()라는 함수로, 또 평균제곱오차가 nn.functional.mse_loss()라는 함수로 구현되어져 있다. 아래는 이번 실습에서 사용할 두 함수의 사용 예제를 간단히 보여준다.\n","\n","``` python\n","import torch.nn as nn\n","model = nn.Linear(input_dim, output_dim)\n","\n","import torch.nn.functional as F\n","cost = F.mse_loss(prediction, y_train)\n","```"]},{"cell_type":"markdown","metadata":{"id":"Ojt-gLCcCC2o"},"source":["### 1. 단순 선형 회귀 구현하기\n","\n","우선 필요한 도구들을 임포트한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACm5xsEoCHgU","executionInfo":{"status":"ok","timestamp":1606890084961,"user_tz":-540,"elapsed":1211,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"6058cecd-3d27-4dec-9292-088de96cd14c"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fdeced8ab10>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"9lAeXplxCL2m"},"source":["이제 데이터를 선언한다. 아래 데이터는 $y=2x$를 가정된 상태에서 만들어진 데이터로 우리는 이미 정답이 W = 2, b = 0임을 알고 있는 상태이다. 모델이 이 두 W와 b의 값을 제대로 찾아내도록 하는 것이 목표이다."]},{"cell_type":"code","metadata":{"id":"XTIPdiefC6kq","executionInfo":{"status":"ok","timestamp":1606890085416,"user_tz":-540,"elapsed":1034,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYfoEjp1DRSf"},"source":["데이터를 정의하였으니 이제 선형 회귀 모델을 구현할 차례이다. nn.Linear()는 입력의 차원, 출력의 차원을 인수로 받는다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RThFj10XDSBd","executionInfo":{"status":"ok","timestamp":1606890086321,"user_tz":-540,"elapsed":1060,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"53742ba6-0f5d-41d7-c203-98fe7e6a84d2"},"source":["# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim = 1, output_dim = 1.\n","model = nn.Linear(1, 1)\n","model"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=1, out_features=1, bias=True)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"GN03-ewIDSDc"},"source":["위 torch.nn.Linear 인자로 1, 1을 사용하였다. 하나의 입력 $x$에 대해서 하나의 출력 $y$를 가지므로, 입력 차원과 출력 차원 모두 1을 인수로 사용하였다. model에는 가중치 W와 편향 b가 저장되어져 있다. 이 값은 model.parameters()라는 함수를 사용하여 불러올 수 있는데, 한 번 출력해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APkZND4hDSFZ","executionInfo":{"status":"ok","timestamp":1606890087484,"user_tz":-540,"elapsed":1577,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"a0d2f45f-7244-4f18-df41-e1b626d4e14b"},"source":["print(list(model.parameters()))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[Parameter containing:\n","tensor([[0.5153]], requires_grad=True), Parameter containing:\n","tensor([-0.4414], requires_grad=True)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oHq3O5kAFidK"},"source":["2개의 값이 출력되는데 첫번째 값이 W고, 두번째 값이 b에 해당된다. 두 값 모두 현재는 랜덤 초기화가 되어져 있다. 그리고 두 값 모두 학습의 대상이므로 requires_grad = True가 되어져 있는 것을 볼 수 있다.\n","\n","이제 옵티마이저를 정의한다. model.parameters()를 사용하여 W와 b를 전달한다.\n","\n","학습률(learning rate)은 0.01로 정한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUX5WFrBGB0i","executionInfo":{"status":"ok","timestamp":1606890087930,"user_tz":-540,"elapsed":1028,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"cf55199a-07fc-4883-fb16-b6616f9116f8"},"source":["# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n","optimizer"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 0.01\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjRjmMjuGB4j","executionInfo":{"status":"ok","timestamp":1606890089589,"user_tz":-540,"elapsed":1104,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"fe94e2f2-2b38-4941-dfa0-49f466e44c23"},"source":["# 전체 훈련 데이터에 대해 경사 하강법을 2000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","        # 100번마다 로그 출력\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n","        "],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch    0/2000 Cost: 13.103541\n","Epoch  100/2000 Cost: 0.002791\n","Epoch  200/2000 Cost: 0.001724\n","Epoch  300/2000 Cost: 0.001066\n","Epoch  400/2000 Cost: 0.000658\n","Epoch  500/2000 Cost: 0.000407\n","Epoch  600/2000 Cost: 0.000251\n","Epoch  700/2000 Cost: 0.000155\n","Epoch  800/2000 Cost: 0.000096\n","Epoch  900/2000 Cost: 0.000059\n","Epoch 1000/2000 Cost: 0.000037\n","Epoch 1100/2000 Cost: 0.000023\n","Epoch 1200/2000 Cost: 0.000014\n","Epoch 1300/2000 Cost: 0.000009\n","Epoch 1400/2000 Cost: 0.000005\n","Epoch 1500/2000 Cost: 0.000003\n","Epoch 1600/2000 Cost: 0.000002\n","Epoch 1700/2000 Cost: 0.000001\n","Epoch 1800/2000 Cost: 0.000001\n","Epoch 1900/2000 Cost: 0.000000\n","Epoch 2000/2000 Cost: 0.000000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"135vMV0NGB-L"},"source":["학습이 완료되었다. Cost의 값이 매우 작다. W와 b의 값도 최적화가 되었는지 확인해보자.\n","\n","$x$에 임의의 값 4를 넣어 모델이 예측하는 $y$의 값을 확인해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnVehl_WGCBo","executionInfo":{"status":"ok","timestamp":1606890217086,"user_tz":-540,"elapsed":1325,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"8de82980-8397-4891-dffe-f8655d2571d4"},"source":["# 임의의 입력 4를 선언\n","new_var = torch.FloatTensor([[4.0]])\n","\n","# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) # forward 연산\n","\n","# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n","print('훈련 후 입력이 4일 때의 예측값 :', pred_y)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kH1ji4raHtbf"},"source":["사실 이 문제의 정답은 $y=2x$가 정답이므로 y값이 8에 가까우면 W와 b의 값이 어느정도 최적화가 된 것으로 볼 수 있다. 실제로 예측된 y값은 7.9989로 8에 매우 가깝다.\n","\n","이제 학습 후의 W와 b의 값을 출력해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AfPi_xAXH_Qe","executionInfo":{"status":"ok","timestamp":1606890296872,"user_tz":-540,"elapsed":1074,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"3bf64c50-6613-47ee-b792-23e4ec802949"},"source":["print(list(model.parameters()))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[Parameter containing:\n","tensor([[1.9994]], requires_grad=True), Parameter containing:\n","tensor([0.0014], requires_grad=True)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sB9VxzH4IBHS"},"source":["W의 값이 2에 가깝고, b의 값이 0에 가까운 것을 볼 수 있다.\n","\n","* $H(x)$ 식에 입력 $x$로부터 예측된 $y$를 얻는 것을 forward 연산이라고 한다.\n","\n","* 학습 전, prediction = model(x_train)은 x_train으로부터 예측값을 리턴하므로 forward 연산이다.\n","\n","* 학습 후, pred_y = model(new_var)는 임의의 값 new_var로부터 예측값을 리턴하므로 forward 연산이다.\n","\n","* 학습 과정에서 비용 함수를 미분하여 기울기를 구하는 것을 backward 연산이라고 한다.\n","\n","* cost.backward()는 비용 함수로부터 기울기를 구하라는 의미이며 backward 연산이다."]},{"cell_type":"markdown","metadata":{"id":"72NkVZ5LImQI"},"source":["### 2. 다중 선형 회귀 구현하기\n","\n","이제 nn.Linear()와 nn.functional.mse_loss()로 다중 선형 회귀를 구현해보자. 사실 코드 자체는 달라지는 건 거의 없는데, nn.Linear()의 인자값과 학습률(learning rate)만 조절해주었다.\n","\n"]},{"cell_type":"code","metadata":{"id":"u7a88Vp_Iy1m","executionInfo":{"status":"ok","timestamp":1606890515478,"user_tz":-540,"elapsed":691,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3Y64kqBI2b6","executionInfo":{"status":"ok","timestamp":1606890521220,"user_tz":-540,"elapsed":1080,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"464fceed-edaf-4abd-de0e-06b8882c24a9"},"source":["torch.manual_seed(1)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fdeced8ab10>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"cOs1p5NFI3yZ"},"source":["이제 데이터를 선언해준다. 여기서는 3개의 $x$로부터 하나의 $y$를 예측하는 문제이다.\n","\n","즉, 가설 수식은 $H(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$이다.\n","\n"]},{"cell_type":"code","metadata":{"id":"0RQcSDGDJAHP","executionInfo":{"status":"ok","timestamp":1606890618743,"user_tz":-540,"elapsed":1441,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAF2snMZJPmx"},"source":["데이터를 정의하였으니 이제 선형 회귀 모델을 구현할 차례이다. nn.Linear()는 입력의 차원, 출력의 차원을 인수로 받는다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GoXiU3OJWgF","executionInfo":{"status":"ok","timestamp":1606890671853,"user_tz":-540,"elapsed":1068,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"20d5302e-0c8a-47ef-e05e-9a77c97fb0b9"},"source":["# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim = 3, output_dim = 1.\n","model = nn.Linear(3, 1)\n","model"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=3, out_features=1, bias=True)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"fKmPkwnoJcoQ"},"source":["위 torch.nn.Linear 인자로 3, 1을 사용하였다. 3개의 입력 x에 대해서 하나의 출력 y를 가지므로, 입력 차원은 3, 출력 차원은 1을 인수로 사용하였다. model에는 3개의 가중치 w와 편향 b가 저장되어져 있다. 이 값은 model.parameters()라는 함수를 사용하여 불러올 수 있는데, 한 번 출력해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeHt6Wi0Jwx0","executionInfo":{"status":"ok","timestamp":1606890762061,"user_tz":-540,"elapsed":1077,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"4ba38271-f445-4851-a05b-eeccc5ec173e"},"source":["print(list(model.parameters()))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[Parameter containing:\n","tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n","tensor([0.2710], requires_grad=True)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e1eRJ1PfJ3Dr"},"source":["첫번째 출력되는 것이 3개의 w이고, 두번째 출력되는 것이 b에 해당된다. 두 값 모두 현재는 랜덤 초기화가 되어져 있다. 그리고 두 출력 결과 모두 학습의 대상이므로 requires_grad = True가 되어져 있는 것을 볼 수 있다.\n","\n","이제 옵티마이저를 정의한다. model.parameters()를 사용하여 3개의 w와 b를 전달한다. 학습률(learning rate)은 0.00001로 정한다. 파이썬 코드로는 1e-5로도 표기한다. 0.01로 하지 않는 이유는 기울기가 발산하기 때문이다. 궁금하면 해보기 바란다.\n","\n","<img src = 'https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG' width = 60%>\n","\n","위의 그림은 앞서 배웠던 내용으로, 학습률(learning rate)이 모델의 필요한 크기보다 높을 때, 기울기가 발산하는 현상을 보여준다.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPnCClkrJ3JY","executionInfo":{"status":"ok","timestamp":1606890945412,"user_tz":-540,"elapsed":1079,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"5210deb0-1768-459e-f046-fd349733d440"},"source":["optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n","optimizer"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 1e-05\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"sa6DTFYmJ3LW"},"source":["이하 코드는 단순 선형 회귀를 구현했을 때와 동일하다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mm2_jDtCJ3Nx","executionInfo":{"status":"ok","timestamp":1606892615880,"user_tz":-540,"elapsed":1542,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"fbc5d874-de45-429a-bca6-8be6d98a176c"},"source":["nb_epochs = 2000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","    # model(x_train)은 model.forward(x_train)과 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","        # 100번마다 로그 출력\n","        print('Epoch: {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch:    0/2000 Cost: 31667.597656\n","Epoch:  100/2000 Cost: 0.225993\n","Epoch:  200/2000 Cost: 0.223911\n","Epoch:  300/2000 Cost: 0.221941\n","Epoch:  400/2000 Cost: 0.220059\n","Epoch:  500/2000 Cost: 0.218271\n","Epoch:  600/2000 Cost: 0.216575\n","Epoch:  700/2000 Cost: 0.214950\n","Epoch:  800/2000 Cost: 0.213413\n","Epoch:  900/2000 Cost: 0.211952\n","Epoch: 1000/2000 Cost: 0.210560\n","Epoch: 1100/2000 Cost: 0.209232\n","Epoch: 1200/2000 Cost: 0.207967\n","Epoch: 1300/2000 Cost: 0.206761\n","Epoch: 1400/2000 Cost: 0.205619\n","Epoch: 1500/2000 Cost: 0.204522\n","Epoch: 1600/2000 Cost: 0.203484\n","Epoch: 1700/2000 Cost: 0.202485\n","Epoch: 1800/2000 Cost: 0.201542\n","Epoch: 1900/2000 Cost: 0.200635\n","Epoch: 2000/2000 Cost: 0.199769\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GMZ5svF4J3Qg"},"source":["학습이 완료되었다. Cost의 값이 매우 작다. 3개의 w와 b의 값도 최적화가 되었는지 확인해보자.\n","\n","$x$에 임의의 입력 [73, 80, 75]를 넣어 모델이 예측하는 $y$의 값을 확인해보겠다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxzm3-X-RQ9e","executionInfo":{"status":"ok","timestamp":1606892803582,"user_tz":-540,"elapsed":1859,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"7abd7e37-3ecb-44bf-db4a-16115dc8020f"},"source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var = torch.FloatTensor([[73, 80, 75]])\n","\n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var)\n","print('훈련 후 입력이 73, 80, 75일 때의 예측값 :', pred_y)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2305]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bgT8-QlgRk3F"},"source":["사실 3개의 값 73, 80, 75는 훈련 데이터로 사용되었던 값이다. 당시 y의 값은 152였는데, 현재 예측값이 151이 나온 것으로 보아 어느정도는 3개의 w와 b의 값이 최적화 된것으로 보인다. 이제 학습 후의 3개의 w와 b의 값을 출력해보겠다.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xt9KRN9PR2C-","executionInfo":{"status":"ok","timestamp":1606892882078,"user_tz":-540,"elapsed":2227,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"099c7885-ab00-4cf7-9b39-2a5034c690a6"},"source":["print(list(model.parameters()))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[Parameter containing:\n","tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n","tensor([0.2802], requires_grad=True)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qXEfndqwR31g"},"source":["https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html#tensorflow-static-graph\n","\n","https://www.geeksforgeeks.org/linear-regression-using-pytorch/\n","\n","https://www.yceffort.kr/2019/02/19/pytorch-02-linear-regression/"]}]}